# Storage(Ceph)

## 基础介绍

Ceph是一个可靠地、自动重均衡、自动恢复的分布式存储系统，根据场景划分可以将Ceph分为三大块，分别是对象存储、块设备存储和文件系统服务。

#### 核心组件

* Ceph OSD, OSD的英文全称是Object Storage Device，它的主要功能是存储数据、复制数据、平衡数据、恢复数据等）
* 伴随OSD的还有一个概念叫做Journal盘，一般写数据到Ceph集群时，都是先将数据写入到Journal盘中，然后每隔一段时间比如5秒再将Journal盘中的数据刷新到文件系统中。一般为了使读写时延更小。
* Ceph Monitor：由该英文名字我们可以知道它是一个监视器，负责监视Ceph集群，维护Ceph集群的健康状态，同时维护着Ceph集群中的各种Map图，Cluster Map是RADOS的关键数据结构，管理集群中的所有成员、关系、属性等信息以及数据的分发。
* Ceph MDS：全称是Ceph MetaData Server，主要保存的文件系统服务的元数据，但对象存储和块存储设备是不需要使用该服务的。

#### Ceph 的基础架构组件

![a](https://images2017.cnblogs.com/blog/1302233/201712/1302233-20171223155452631-121429135.jpg)

如图，最底层的RADOS为Ceph提供了高可靠、高可拓展、高性能、高自动化等特性，用户数据的存储最终也都是通过这一层来进行存储的，是Ceph系统的核心。RADOS 层由 OSD 和 Monitor 组成，分别对应核心组件中的 Cepg OSD 和 Ceph Monitor。
  
## Ceph 数据分布算法：CRUSH

在分布式存储系统中比较关注的一点是如何使得数据能够分布得更加均衡，常见的数据分布算法有一致性Hash和Ceph的Crush算法。Crush是一种伪随机的控制数据分布、复制的算法，Ceph是为大规模分布式存储而设计的，数据分布算法必须能够满足在大规模的集群下数据依然能够快速的准确的计算存放位置，同时能够在硬件故障或扩展硬件设备时做到尽可能小的数据迁移，Ceph的CRUSH算法就是精心为这些特性设计的，可以说CRUSH算法也是Ceph的核心之一。

#### 概念说明

* 当用户要将数据存储到Ceph集群时，存储数据都会被分割成多个object，每个object都有一个object id，每个object的大小是可以设置的，默认是4MB，object可以看成是Ceph存储的最小存储单元。
* 由于object的数量很多，所以Ceph引入了pg的概念用于管理object，每个object最后都会通过CRUSH计算映射到某个pg中，一个pg可以包含多个object。
* pg也需要通过CRUSH计算映射到osd中去存储，如果是二副本的，则每个pg都会映射到二个osd，比如[osd.1,osd.2]，那么osd.1是存放该pg的主副本，osd.2是存放该pg的从副本，保证了数据的冗余。
* pg是用来存放object的，pgp相当于是pg存放osd的一种排列组合

以上概念的关系图如下：

![b](https://images2017.cnblogs.com/blog/1302233/201712/1302233-20171224011812365-1637840216.png)

数据的写入流程为：经过PG这一层预先定义好的定额Hash分片，然后PG，再经过一次集群所有物理机器硬盘OSD构成的Hash，落到物理磁盘。

![c](http://5b0988e595225.cdn.sohucs.com/images/20180608/9990ca370fb043dabf0575f6c230d309.jpeg)

## 优点与缺点

#### 可扩展性
Ceph声称可以无限扩展，因为它基于CRUSH算法，没有中心节点。CRUSH在一致性哈希基础上很好的考虑了容灾域的隔离，能够实现各类负载的副本放置规则，例如跨机房、机架感知等。同时， CRUSH算法支持副本和EC两种数据冗余方式，还提供了四种不同类型的Bucket(Uniform, List, Tree, Straw)，充分考虑了实际生产过程中硬件的迭代式部署方式，而事实上，Ceph确实可以无限扩展，但Ceph的无限扩展的过程，并不完全美好。

* 扩容粒度小。在实践中，扩容受“容错域”制约，一次只能扩一个“容错域”。这样将导致扩容粒度小。而数据的日均增长量是很有可能大于一台机器的存储容量的。这就会造成扩容速度赶不上写入速度的尴尬局面。这对于开始没有设计好，图快速deploy而架设的集群，在后期是一个不小的伤害。
* Crushmap变更。Ceph是根据crushmap去放置PG的物理位置的。扩容时 crushmap 因硬盘损坏而改变，扩容进度被迫进行了很久才回到稳定的状态。
* 扩容到一定量级后，PG数量需调整。几乎50%的对象，涉及到调整后的PG都需要重新放置物理位置，这会引起服务质量的严重下降。
  
#### 代码质量

Ceph主要使用C/C++语言编写，同时外围的很多脚本和工具用了Python。之所以要说明Ceph的语言构成，是因为代码质量实际上是和语言具有密切的关系。不否认用C++也能写出优雅的代码，但相比于更加“现代”的语言，要想写出具备同样可读性、结构良好、调理清晰代码，C++要困难很多。
> 在目前的master分支上，相关文件的大小分别是：
OSD.h+OSD.cc = 2383行+8604行 = 10987行
PG.h+PG.cc = 2256行+7611行 = 9867行
ReplicatedPG.h+ReplicatedPG.cc = 1487行+12665行 = 14152行

但是，由于存储作为底层系统，对效率的追求是无止境的，因此不太可能舍弃对于内存等底层系统资源的控制，而使用 Java/Python这类的语言。而作为一个开源项目，期望所有的贡献者都是C++的高手，未免有些强人所难，这似乎成了一个死结。

#### 性能隐患

* 数据双倍写入。Ceph本地存储接口(FileStore)为了支持事务，引入了日志(Journal)机制。所有的写入操作都需要先写入日志(XFS模式下)，然后再写入本地文件系统。简单来说就是一份数据需要写两遍，日志+本地文件系统。这就造成了在大规模连续IO的情况下，实际上磁盘输出的吞吐量只有其物理性能的一半。
* IO路径过长。这个问题在Ceph的客户端和服务器端都存在。以osd为例，一个IO需要经过message、OSD、FileJournal、FileStore多个模块才能完成，每个模块之间都涉及到队列和线程切换，部分模块在对IO进行处理时还要进行内存拷贝，导致整体性能不高。
* 对高性能硬件的支持有待改进。Ceph最开始是为HDD设计的，没有充分考虑全SSD，甚至更先进的PCIe SSD和NVRAM的情况NVRAM。导致这些硬件的物理性能在Ceph中无法充分发挥出来，特别是延迟和IOPS，受比较大的影响。

## 思考

1. 系统设计之初应当考虑到日后可能出现的变化，同时为可能出现的变化做好准备。这需要系统设计者有较为长远的目光和丰富的系统设计经验。
2. 设计系统时需要尽可能简化系统的设计。如果系统中某个部件对于系统的好处大于坏处时，才应当将该部件添加至系统中，否则将会增加系统的复杂性。
  
## 参考文献

[历经十年：关于Ceph现状与未来的一些思考](http://cloud.51cto.com/art/201505/474992.htm)

[分布式文件系统Ceph面面观](http://cloud.51cto.com/art/201505/478170.htm)

[Ceph基础知识和基础架构认识](https://www.cnblogs.com/luohaixian/p/8087591.html)

[Ceph运维告诉你分布式存储的那些“坑”](http://www.sohu.com/a/234562423_411876)